import torch
import os
import sys
from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig
from peft import PeftModel

# –ò–º–ø–æ—Ä—Ç–∏—Ä—É–µ–º –Ω–∞—Å—Ç—Ä–æ–π–∫–∏ –∏–∑ –≤–∞—à–µ–≥–æ –∫–æ–Ω—Ñ–∏–≥–∞, —á—Ç–æ–±—ã –Ω–µ –¥—É–±–ª–∏—Ä–æ–≤–∞—Ç—å –∏—Ö
from config import MODEL_ID, OUTPUT_DIR, MAX_SEQ_LENGTH, SYSTEM_PROMPT, COMPUTE_DTYPE

def load_model_and_tokenizer():
    """–ó–∞–≥—Ä—É–∂–∞–µ—Ç –º–æ–¥–µ–ª—å –∏ –∞–¥–∞–ø—Ç–µ—Ä (–∞–Ω–∞–ª–æ–≥–∏—á–Ω–æ startup —Å–æ–±—ã—Ç–∏—é –≤ app.py)."""
    
    adapter_path = os.path.join(OUTPUT_DIR, "final_adapter")
    
    if not os.path.exists(adapter_path):
        print(f"–û—à–∏–±–∫–∞: –ê–¥–∞–ø—Ç–µ—Ä –Ω–µ –Ω–∞–π–¥–µ–Ω –ø–æ –ø—É—Ç–∏ {adapter_path}")
        print("–°–Ω–∞—á–∞–ª–∞ –∑–∞–ø—É—Å—Ç–∏—Ç–µ train.py!")
        sys.exit(1)

    print(f"1. –ó–∞–≥—Ä—É–∑–∫–∞ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä–∞: {MODEL_ID}...")
    tokenizer = AutoTokenizer.from_pretrained(MODEL_ID, trust_remote_code=True)
    if tokenizer.pad_token is None:
        tokenizer.pad_token = tokenizer.eos_token

    print("2. –ù–∞—Å—Ç—Ä–æ–π–∫–∞ 4-bit –∫–≤–∞–Ω—Ç–æ–≤–∞–Ω–∏—è...")
    bnb_config = BitsAndBytesConfig(
        load_in_4bit=True,
        bnb_4bit_quant_type="nf4",
        bnb_4bit_compute_dtype=COMPUTE_DTYPE,
        bnb_4bit_use_double_quant=False,
    )

    print(f"3. –ó–∞–≥—Ä—É–∑–∫–∞ –±–∞–∑–æ–≤–æ–π –º–æ–¥–µ–ª–∏...")
    model = AutoModelForCausalLM.from_pretrained(
        MODEL_ID,
        quantization_config=bnb_config,
        device_map="auto",
        trust_remote_code=True,
    )

    print(f"4. –ü–æ–¥–∫–ª—é—á–µ–Ω–∏–µ LoRA –∞–¥–∞–ø—Ç–µ—Ä–∞ –∏–∑ {adapter_path}...")
    model = PeftModel.from_pretrained(model, adapter_path)
    
    # –í–ª–∏–≤–∞–µ–º –∞–¥–∞–ø—Ç–µ—Ä –≤ –≤–µ—Å–∞ –º–æ–¥–µ–ª–∏ –¥–ª—è —É—Å–∫–æ—Ä–µ–Ω–∏—è –∏–Ω—Ñ–µ—Ä–µ–Ω—Å–∞
    model = model.merge_and_unload()
    model.eval()
    
    return model, tokenizer

def chat_loop(model, tokenizer):
    """–ó–∞–ø—É—Å–∫–∞–µ—Ç —Ü–∏–∫–ª —á–∞—Ç–∞ –≤ –∫–æ–Ω—Å–æ–ª–∏."""
    
    # –ò—Å—Ç–æ—Ä–∏—è –¥–∏–∞–ª–æ–≥–∞
    history = []
    
    # –ù–∞—Å—Ç—Ä–∞–∏–≤–∞–µ–º —Å—Ç–æ–ø-—Ç–æ–∫–µ–Ω—ã –¥–ª—è Qwen
    terminators = [
        tokenizer.eos_token_id,
        tokenizer.convert_tokens_to_ids("<|im_end|>"),
        tokenizer.convert_tokens_to_ids("<|endoftext|>")
    ]

    print("\n" + "="*50)
    print("ü§ñ –ú–æ–¥–µ–ª—å –≥–æ—Ç–æ–≤–∞! –ü–∏—à–∏—Ç–µ –≤–∞—à –≤–æ–ø—Ä–æ—Å (–∏–ª–∏ 'exit' –¥–ª—è –≤—ã—Ö–æ–¥–∞).")
    print("="*50 + "\n")

    while True:
        try:
            user_input = input("\033[1;34mUser:\033[0m ").strip()
            
            if user_input.lower() in ["exit", "quit", "–≤—ã—Ö–æ–¥"]:
                print("–ó–∞–≤–µ—Ä—à–µ–Ω–∏–µ —Ä–∞–±–æ—Ç—ã.")
                break
            
            if not user_input:
                continue

            # –§–æ—Ä–º–∏—Ä—É–µ–º —Å–æ–æ–±—â–µ–Ω–∏—è —Å —É—á–µ—Ç–æ–º –∏—Å—Ç–æ—Ä–∏–∏
            messages = [{"role": "system", "content": SYSTEM_PROMPT}]
            messages.extend(history)
            messages.append({"role": "user", "content": user_input})

            # –ü—Ä–∏–º–µ–Ω—è–µ–º Chat Template
            text_input = tokenizer.apply_chat_template(
                messages, 
                tokenize=False, 
                add_generation_prompt=True
            )
            
            inputs = tokenizer(
                text_input, 
                return_tensors="pt", 
                add_special_tokens=False
            )
            
            model_inputs = inputs.to(model.device)

            print("\033[1;33mAssistant –¥—É–º–∞–µ—Ç...\033[0m", end="\r")

            with torch.no_grad():
                generated_ids = model.generate(
                    **model_inputs,
                    max_new_tokens=1024,
                    do_sample=True,
                    temperature=0.6,
                    top_p=0.9,
                    top_k=50,
                    eos_token_id=terminators,
                    pad_token_id=tokenizer.pad_token_id,
                    use_cache=True # –î–ª—è –∏–Ω—Ñ–µ—Ä–µ–Ω—Å–∞ –∫—ç—à –ø–æ–ª–µ–∑–µ–Ω
                )

            # –î–µ–∫–æ–¥–∏—Ä—É–µ–º —Ç–æ–ª—å–∫–æ –Ω–æ–≤—ã–µ —Ç–æ–∫–µ–Ω—ã
            input_len = model_inputs.input_ids.shape[1]
            generated_ids = generated_ids[:, input_len:]
            response = tokenizer.decode(generated_ids[0], skip_special_tokens=False)

            # –û—á–∏—Å—Ç–∫–∞ –æ—Ç –∞—Ä—Ç–µ—Ñ–∞–∫—Ç–æ–≤
            response = response.replace("<|im_end|>", "").replace("<|endoftext|>", "").strip()

            print(f"\033[1;32mAssistant:\033[0m {response}\n")
            print("-" * 30)

            # –û–±–Ω–æ–≤–ª—è–µ–º –∏—Å—Ç–æ—Ä–∏—é
            history.append({"role": "user", "content": user_input})
            history.append({"role": "assistant", "content": response})
            
            # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –∏—Å—Ç–æ—Ä–∏—é (–Ω–∞–ø—Ä–∏–º–µ—Ä, –ø–æ—Å–ª–µ–¥–Ω–∏–µ 10 —Å–æ–æ–±—â–µ–Ω–∏–π), —á—Ç–æ–±—ã –Ω–µ –ø–µ—Ä–µ–ø–æ–ª–Ω–∏—Ç—å –∫–æ–Ω—Ç–µ–∫—Å—Ç
            if len(history) > 10:
                history = history[-10:]

        except KeyboardInterrupt:
            print("\n–ü—Ä–µ—Ä–≤–∞–Ω–æ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–º.")
            break
        except Exception as e:
            print(f"\n–û—à–∏–±–∫–∞: {e}")

if __name__ == "__main__":
    model, tokenizer = load_model_and_tokenizer()
    chat_loop(model, tokenizer)